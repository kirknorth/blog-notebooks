{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<h1> Derivative Approximation by Finite Differences </h1>\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to be able to approximate derivatives of a multivariate function $f(x,y,z)$ by finite differences. More precisely, we want to approximate the $d$-th order derivative of $f$ given a small value $h \\gt 0$ for an integer order of error $p \\gt 0$. This is typically done by making use of the Taylor expansion of $f$ around $x$, $y$, and $z$. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "from fractions import Fraction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Univariate functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First let's deal with the more simplified case of a univariate function $f(x)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " Given a small value $h \\gt 0$, the $d$-th order derivative of $f$ satisfies the following equation, where the integer order of error $p \\gt 0$ may be selected as desired,\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{h^d}{d!} f^{(d)} (x) + \\mathcal{O}(h^{d+p}) = \\sum_{i=i_o}^{i_f} C_i f(x+ih).\n",
      "\\end{equation}\n",
      "    \n",
      "Here $i_o$ and $i_f$ are the min and max $i$-th <i>grid points</i> or <i>nodes</i> used in the derivative calculation. The equation becomes approximate by neglecting the $\\mathcal{O}(h^{d+p})$ term. The vector $\\mathbf{c} = (C_{i_o},C_{{i_o}+1}...,C_{i_f})$ is called the <i>template</i> or <i>stencil</i> for the approximation.\n",
      "\n",
      "The general Taylor expansion of $f$ around $x$ is,\n",
      "\n",
      "\\begin{equation}\n",
      "f(x+ih) = \\sum_{n=0}^{\\infty} \\frac{(ih)^n}{n!} f^{(n)} (x).\n",
      "\\end{equation}\n",
      "\n",
      "For example, the Taylor series of $f(x+h)$, that is when $i = 1$, is,\n",
      "\n",
      "\\begin{equation}\n",
      "f(x+h) = f(x) + hf^\\prime (x) + \\frac{h^2}{2!}f^{\\prime\\prime}(x) + \\mathcal{O}(h^3)\n",
      "\\end{equation}\n",
      "\n",
      "This gives rise to the classic forward difference equation for $f^\\prime (x)$, which is $\\mathcal{O}(h)$. Substituting equation (2) into (1) yields,\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{h^d}{d!} f^{(d)} (x) + \\mathcal{O}(h^{d+p}) = \\sum_{i=i_o}^{i_f} C_i \\sum_{n=0}^{\\infty} \\frac{(ih)^n}{n!} f^{(n)} (x).\n",
      "\\end{equation}\n",
      "\n",
      "Dividing both sides by $\\frac{h^d}{d!}$ and moving the order term to the right hand side yields,\n",
      "\n",
      "\\begin{equation}\n",
      "f^{(d)} (x) = \\frac{d!}{h^d} \\sum_{i=i_o}^{i_f} C_i \\sum_{n=0}^{\\infty} \\frac{(ih)^n}{n!} f^{(n)} (x) + \\mathcal{O}(h^{p}),\n",
      "\\end{equation}\n",
      "\n",
      "which is an equation that expresses the $d$-th order derivative in terms of the integer order of error $p$, exactly what we're looking for. Here we note that in the approximation $n$ no longer goes to infinity, but instead $d + p - 1$. This can be seen by the following two examples. For the forward difference scheme, we approximate the first derivate ($d = 1$) to first-order accuracy ($p = 1$), which requires $n = 0, 1$ terms of the Taylor expansion. For a second-order accurate ($p = 2$) centered difference scheme of the first derivative ($d = 1$), we require $n = 0,1,2$ terms of the Taylor expansion. Therefore we now have,\n",
      "\n",
      "\\begin{equation}\n",
      "f^{(d)} (x) =  \\frac{d!}{h^d} \\sum_{n=0}^{d+p-1} \\left( \\sum_{i=i_o}^{i_f} i^n C_i \\right) \\frac{h^n}{n!} f^{(n)} (x) + \\mathcal{O}(h^{p}),\n",
      "\\end{equation}\n",
      "\n",
      "The above equation is satisfied when $n = d$, which therefore requires that the term in brackets vanish for when $n \\ne d$, or mathematically,\n",
      "\n",
      "\\begin{equation}\n",
      "\\sum_{i=i_o}^{i_f} i^n C_i =\n",
      "\\begin{cases}\n",
      "0, & \\text{if } n \\ne d \\\\\n",
      "1, & \\text{if } n = d\n",
      "\\end{cases}\n",
      "\\end{equation}\n",
      "\n",
      "So we have a set of $d + p$ linear equations in $i_f - i_o + 1$ unknowns. Therefore if we constrain the number of unkowns to be $d + p$, the linear system has a unique solution. Therefore, any <i>forward difference</i> scheme has $i_o = 0$ and $i_f = d + p - 1$, any <i> backward difference</i> scheme has $i_o = -(d + p -1)$ and $i_f = 0$, and any <i>centered difference</i> scheme has $i_f = -i_f = \\frac{d + p - 1}{2}$. For the centered difference schemes, if $d + p$ <i>is not odd</i>, then the system is <i>overdetermined</i> because $d + p \\gt i_f - i_o + 1$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Examples"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Approximate $f^\\prime(x)$ with a forward difference with error $\\mathcal{O}(h^2)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we note that $d = 1$ and $p = 2$, and so for a forward difference we need $i_o = 0$ and $i_f = 2$. Therefore we need to solve the following system of linear equations,\n",
      "\n",
      "\\begin{equation}\n",
      "\\begin{bmatrix}\n",
      "1 & 1 & 1 \\\\\n",
      "0 & 1 & 2 \\\\\n",
      "0 & 1 & 4 \\\\\n",
      "\\end{bmatrix} \n",
      "\\begin{bmatrix}\n",
      "C_0 \\\\\n",
      "C_1 \\\\\n",
      "C_2 \\\\\n",
      "\\end{bmatrix} = \n",
      "\\begin{bmatrix}\n",
      "0 \\\\\n",
      "1 \\\\\n",
      "0 \\\\\n",
      "\\end{bmatrix}.\n",
      "\\end{equation}\n",
      "\n",
      "We'll write a Python function to help us find the solution to this matrix equation given $d$ and $p$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def univariate_coefficients(d, p, scheme='forward', verbose=True):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    \n",
      "    # Compute appropriate nodes for specified scheme\n",
      "    if scheme == 'forward':\n",
      "        i_o = 0\n",
      "        i_f = d + p - 1\n",
      "    if scheme == 'backward':\n",
      "        i_o = -(d + p - 1)\n",
      "        i_f = 0\n",
      "    if scheme == 'centered':\n",
      "        i_f = (d + p - 1) / 2\n",
      "        i_o = -i_f\n",
      "    \n",
      "    # Fill a and b list\n",
      "    a = [[i**n for i in xrange(i_o, i_f+1)] for n in xrange(d+p)]\n",
      "    b = [0**(abs(n-d)) for n in xrange(d+p)]\n",
      "    \n",
      "    # Convert lists to NumPy arrays\n",
      "    a = np.array(a, dtype='float64', ndmin=2)\n",
      "    b = np.array(b, dtype='float64', ndmin=1)\n",
      "    \n",
      "    # Solve ax = b\n",
      "    x, resid, rank, s = np.linalg.lstsq(a, b)\n",
      "    \n",
      "    # Check for floating point arithmetic issues and\n",
      "    # limitations. The a priori here is that we expect \n",
      "    # rational numbers\n",
      "    x = np.where(np.abs(x) < 1.0e-12, 0.0, x) \n",
      "    \n",
      "    # Print solution\n",
      "    if verbose:\n",
      "        for i, c in enumerate(x): \n",
      "            print 'C_%d = %s' %(i+i_o, Fraction(str(c)).limit_denominator(10000))\n",
      "    \n",
      "    return x, i_o, i_f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate coefficients\n",
      "c, i_o, i_f = univariate_coefficients(1, 2, scheme='forward')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C_0 = -3/2\n",
        "C_1 = 2\n",
        "C_2 = -1/2\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore, from equation (1), $f^\\prime(x)$ can be approximated by a forward difference to $\\mathcal{O}(h^2)$ as,\n",
      "\n",
      "\\begin{equation}\n",
      "f^\\prime(x) = \\frac{-f(x+2h) + 4f(x+h) - 3f(x)}{2h} + \\mathcal{O}(h^2).\n",
      "\\end{equation}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Approximate $f^2(x)$ with a backward difference with error $\\mathcal{O}(h^2)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we note that $d = 2$, $p = 2$, $i_o = -3$, and $i_f = 0$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c, i_o, i_f = univariate_coefficients(2, 2, scheme='backward')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C_-3 = -1/2\n",
        "C_-2 = 2\n",
        "C_-1 = -5/2\n",
        "C_0 = 1\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore, from equation (1), $f^{\\prime\\prime}(x)$ can be approximated by a backward difference to $\\mathcal{O}(h^2)$ as,\n",
      "\n",
      "\\begin{equation}\n",
      "f^{\\prime\\prime}(x) = \\frac{-f(x-3h) + 4f(x-2h) - 5f(x-h) - 2f(x)}{h^2} + \\mathcal{O}(h^2).\n",
      "\\end{equation}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Final note"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The discussion above deals with univariate functions, e.g. $f = f(x)$. However, the same techniques are applicable to <i>multivariate</i> functions (e.g. $f = f(x,y,z)$) if and only if you're dealing with <i>partial derivatives</i> of <i>single order</i>. As an example, you could use the above technique to approximate $f_{xx}$ but not $f_{xy}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bivariate functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we deal with functions of two variables, e.g. $f = f(x,y)$. As stated before, derivatives such as $f_x$, $f_y$, and $f_{xx}$ just use the univariate approximation formulas. The mixed derivatives (e.g. $f_{xy}$) require slightly more work, however the underlying process is that for $f_{xy}$ we first apply the <i>x</i> derivative approximation $f_{x}$ and then apply the <i>y</i> derivative approximation $f_{y}$ to the previous approximation.\n",
      "\n",
      "Given small $h \\gt 0$ and $k \\gt 0$, the $m$ and $n$-th mixed-order derivative of $f$ satisfies the following equation, with integer orders of error $p \\gt 0$ and $q \\gt 0$,\n",
      "\n",
      "\\begin{equation}\n",
      "\\frac{k^n}{n!} \\frac{\\partial^n}{\\partial y^n} \\frac{h^m}{m!} \\frac{\\partial^m}{\\partial x^m} f(x,y) + \\mathcal{O}(h^{m+p}, k^{n+q}) = \\sum_{i=i_o}^{i_f} \\sum_{j=j_o}^{j_f} C_i^m C_j^n f(x+ih,y+jk)\n",
      "\\end{equation}\n",
      "\n",
      "Here $i_o$ and $i_f$ are the min and max $i$-th nodes used in the $x$ derivative calculation, and $j_o$ and $j_f$ are the min and max $j$-th nodes used the the $y$ derivative calculation. Therefore, the full stencil of the approximation is given by the <i>tensor product</i> of $C_i$ and $C_j$. In this case the tensor product is simply the outer product because $C_i$ and $C_j$ are vectors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Examples"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Approximate $f_{xy}$ with a centered difference in both $x$ and $y$ to $\\mathcal{O}(h^2,k^2)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we note that $m = 1$, $n = 1$, $p = 2$, $q = 2$, $i_f = -i_o = -1$, and $j_f = -j_o = -1$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bivariate_coefficients(ci, cj, i_o, j_o, verbose=True):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    \n",
      "    # Use NumPY matrix class\n",
      "    ci = np.matrix(ci)\n",
      "    cj = np.matrix(cj)\n",
      "    \n",
      "    # Compute outer product\n",
      "    c = ci.T * cj\n",
      "    \n",
      "    # Print coefficients\n",
      "    if verbose:\n",
      "        for i in xrange(c.shape[0]):\n",
      "            for j in xrange(c.shape[1]):\n",
      "                print 'C_%d_%d = %s' %(i+i_o, j+j_o, Fraction(str(c[i,j])).limit_denominator(10000))\n",
      "    \n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ci, i_o, i_f = univariate_coefficients(1, 2, scheme='centered', verbose=False)\n",
      "cj, j_o, j_f = univariate_coefficients(1, 2, scheme='centered', verbose=False)\n",
      "c = bivariate_coefficients(ci, cj, i_o, j_o)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C_-1_-1 = 1/4\n",
        "C_-1_0 = 0\n",
        "C_-1_1 = -1/4\n",
        "C_0_-1 = 0\n",
        "C_0_0 = 0\n",
        "C_0_1 = 0\n",
        "C_1_-1 = -1/4\n",
        "C_1_0 = 0\n",
        "C_1_1 = 1/4\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore, from equation (3), $f_{xy}$ can be approximated by a forward difference to $\\mathcal{O}(h^2, k^2)$ as,\n",
      "\n",
      "\\begin{equation}\n",
      "f_{xy} = \\frac{f(x+h,y+k) - f(x+h,y-k) - f(x-h,y+k) + f(x-h,y-k)}{4hk} + \\mathcal{O}(h^2, k^2).\n",
      "\\end{equation}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ci, i_o, i_f = univariate_coefficients(1, 1, scheme='backward', verbose=False)\n",
      "cj, j_o, j_f = univariate_coefficients(1, 1, scheme='backward', verbose=False)\n",
      "c = bivariate_coefficients(ci, cj, i_o, j_o)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C_-1_-1 = 1\n",
        "C_-1_0 = -1\n",
        "C_0_-1 = -1\n",
        "C_0_0 = 1\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ci, i_o, i_f = univariate_coefficients(1, 1, scheme='backward', verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C_-1 = -1\n",
        "C_0 = 1\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}